![image](https://user-images.githubusercontent.com/88874870/147899388-b55b9de3-ce2d-4ff8-a20d-5020e5244375.png)
## KoGPT2를 활용한 자기소개서 문장 생성기
## Speech Recognizer를 활용한 모의면접

# 1. 프로젝트 선정 개요
해마다 취업문은 좁아지고 취업준비생의 부담은 더 가중되고 있다. 취업의 첫 관문인 자기소개서를 작성하는 것이 쉽지 않기 때문이다. 한 취업 사이트 통계에 따르면 자기소개서 작성에 극심한 어려움을 느끼는 이른바 ‘자소서 포비아’를 겪는 구직자도 10명 중 8명인 것으로 집계됐다. 이러한 어려움을 해결하는 데 도움을 줄 수 있는 애플리케이션을 기획했다.

첫 번째로 자기소개서 작성 시 막히는 부분에서 키워드를 입력하면 AI가 문장을 작성해주는 서비스를 자연어 생성(NLP) 모델 중 KoGPT2를 선정하여 약10만개의 합격 자기소개서를 학습 후 어색하거나 문법에 맞지 않는 문장을 지양하고 사용자가 원하는 답변에 근접하도록 샘플링하여 활용하였습니다.

두 번째로 면접에 어려움을 느끼는 분들을 위한 면접트레이닝을 Android Speech Recognizer를 활용하여 실제 모의면접처럼 음성으로 질문이 전달되고 음성으로 답변한 부분을 텍스트로 지원하여 자신의 부족한 부분을 점검 및 보안 할 수 있도록 하였습니다.

마지막으로 서버를 통한 사용자의 로그인 정보를 바탕으로 DB에 저장된 본인의 자기소개서, 모의 면접의 저장된 기록들을 확인, 수정, 공유 기능을 통해 취업 준비에 실질적인 도움을 줄 수 있는 서비스 구현을 목표로 하였습니다.


![image](https://user-images.githubusercontent.com/88874870/147899493-9d694e7e-623e-415b-b0fa-5a76c30fba30.png)

# 2. 프로젝트 범위 및 구성
![image](https://user-images.githubusercontent.com/88874870/147899667-3c14c569-9300-46e7-b188-ac38e5bb7b72.png)


# 3. 프로젝트 결과
 *  3.1 모델 아키텍처 (Model Architecture)
 *  
![image](https://user-images.githubusercontent.com/88874870/147899748-bda448ce-b657-49b2-a626-c056f14cb7c4.png)

[모델선정]
여러 자연어 모델 중, 자연어 생성 모델 중에서 좋은 성능을 보이고 있고 한국어가 pretrained 되어있으며, 올해 5월에 version-2를 선보인 KoGPT2모델로 선정하였다. KoGPT2 모델은 한국어 위키백과 이외, 뉴스, 모두의 말뭉치 v1.0, 청와대 국민청원 등 다양한 데이터가 학습되어 있다.

[Crawling]
Linkareer.com 사이트의 합격자소서 10,522건 전체를 크롤링 하였다. xpath를 이용하여 한 페이지 안의 20건의 자소서를 모두 크롤링 후 다음 페이지로 이동하여 크롤링 하는 방식으로 진행하였다.

[Data Processing]
1차 전처리 : 정규식을 이용하여 공통되는 부분을 삭제 후, readlines을 이용해 문장 단위 자르기
2차 전처리 : 자기소개서 질문과 소제목은 수작업으로 제거하였다. 우선, 엑셀의 중복된 항목 제거로 완벽히 똑같은 문장이 있을시 제거 후, 메모장을 이용하여 수기 제거 작업을 하였다. 한자와 정치적 편향이 심한 내용 모두 제거하였다. 전처리 후 183,854줄로 전처리를 완료하였다.

[Training]
전처리된 데이터를 기반으로 fine-tuning training을 진행하였다. batch size는 10에서 100까지 사이의 값을 넣었을 때 가장 효과가 있는 것으로 알려져 있어, 17로 배치사이즈를 지정하였다. 우선 1epoch 실행후, 5epoch, 15epoch를 순차적으로 진행하여 time, loss, 성능을 비교할 수 있도록 하였다. 최종 15epoch로 학습했을 시, 총 31시간 소요, 2.17까지 loss를 낮추었다.

[Text Generation]
앞서 train된 model, tokenizer를 가져와서 샘플링 후 문장을 생성시켰다. k개의 토큰을 뽑는 top_k는 40으로, 누적 분포 값으로 계산하는 top_p는 0.95로 정확한 답변을 얻을 수 있도록 샘플링 하였다.



